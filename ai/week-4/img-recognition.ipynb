{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First AI Project: Recognising Handwritten Numbers with PyTorch\n",
    "_Written by Yiding Song for Team Enigma. Licensed under the MIT License._\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/harrow-css/2023-multithreading/blob/main/ai/week-4/img-recognition.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pytorch.org/) is an open-source [Python](https://www.python.org/) library that makes designing and training neural networks very easy. Other libraries like [TensorFlow](https://www.tensorflow.org/) and [Flax](https://flax.readthedocs.io/) exist as well, but we're going to stick with PyTorch because it's really easy to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing PyTorch and helpful modules\n",
    "import torch\n",
    "from torch import nn    # nn is PyTorch's neural network module\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Other helpful libraries for visualisation\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes you'd want to run your model on a GPU to boost performance.\n",
    "# The following code finds the right device to execute your code on\n",
    "# depending on hardware\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: batch size\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The machine learning worflow**\n",
    "\n",
    "<img src=\"https://www.mermaidchart.com/raw/c07f9f84-515d-4807-9d54-0011a6bc6e67?theme=light&version=v0.1&format=svg\" style=\"height: 500px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: MNIST\n",
    "\n",
    "We are going to use handwritten digits from the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) in order to train our model. It's one of the earliest machine learning benchmarks, and is also the first AI system I built!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data using torchvision datasets\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualise dataset; don't worry too much\n",
    "\n",
    "def visualise_samples(data, n_rows=5, n_cols=5, figsize=(8, 8)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(1, n_rows*n_cols+1):\n",
    "        img, lab = data[i]\n",
    "        fig.add_subplot(n_rows, n_cols, i)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Label: {lab}')\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_samples(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data into batches, so that model can learn from multiple\n",
    "# data points at once!\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Here we're going to define our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=512, activation_fn=nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(28*28, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, hidden)\n",
    "        self.linear3 = nn.Linear(hidden, 10)\n",
    "        \n",
    "        self.activation = activation_fn()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.activation(self.linear2(x))\n",
    "        logits = self.activation(self.linear3(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_data[0][0].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimisation\n",
    "\n",
    "This is where we're going to take care of gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperperamters\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, y, model, loss_fn, optim):\n",
    "\n",
    "    pred = model(x)         # Compute model output\n",
    "    loss = loss_fn(pred, y) # Compute loss with respect to target output\n",
    "    \n",
    "    loss.backward()         # Backpropagate gradients through model\n",
    "    optim.step()            # Use gradient to update model weights\n",
    "    optim.zero_grad()       # Reset gradients (just a PyTorch thing)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optim):\n",
    "    # Training mode for batchnorm, dropout, etc.\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(dataloader) as pbar:\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = train_step(x, y, model, loss_fn, optim)\n",
    "            pbar.set_postfix({'loss': f'{loss:>7f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    '''\n",
    "    Code taken from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "    '''\n",
    "    \n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!!!\n",
    "\n",
    "Optimise model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {ep+1}:\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optim)\n",
    "    test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have something that works, can you improve model accuracy on the test set?\n",
    "\n",
    "**The person with best test set accuracy by Nov 15 2023 (Wed) will receive a Send-Up!** Submit your solution by downloading your code (both `.py` and `.ipynb` is fine) and sending it to me. If you're on Colab, you can download your code by clicking `File` --> `Download` --> `Download .ipynb`.\n",
    "\n",
    "A few things you might want to play around with:\n",
    "\n",
    "* Finetuning hyperparameters: e.g. `BATCH_SIZE`, `LEARNING_RATE`, `EPOCHS`.\n",
    "* Switch the optimiser algorithm: for more information about optimisers, check out [this](https://dev.to/amananandrai/10-famous-machine-learning-optimizers-1e22) and [this](https://pytorch.org/docs/stable/optim.html).\n",
    "* Change the model architecture: change the number of layers, number of hidden units, type of layer used, etc... Check out [this](https://www.bmc.com/blogs/machine-learning-architecture/) and [this](https://pytorch.org/docs/stable/nn.html) for alternative model architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
