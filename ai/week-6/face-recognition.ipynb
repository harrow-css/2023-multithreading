{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zna-MjBQ9eSV"
      },
      "source": [
        "# Face Recognition\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/harrow-css/2023-multithreading/blob/main/ai/week-6/face-recognition.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Train a basic face recognition model using convolutional neural networks!\n",
        "\n",
        "![ConvNet](https://miro.medium.com/max/3944/1*YejW73f36BGhNGhrtbz67g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npEmAcX6Urbe"
      },
      "source": [
        "Copyright 2023 Team Enigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q23TvepqjKGZ"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Team Enigma\n",
        "\n",
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5vQ6Ywdlbke"
      },
      "source": [
        "## Importing libraries\n",
        "\n",
        "We will be using [TensorFlow](https://tensorflow.org/), a great Machine Learning library from Google!\n",
        "\n",
        "**Spoiler Alert!**\n",
        "\n",
        "Creating a machine learning project from scratch takes a lot of time, and gets you stuck into nitty-gritty details, instead of the more interesting high-level overview (which is what this lecture series is all about!). Most machine learning researchers nowadays use standard library like TensorFlow and PyTorch anyways, so there's nothing embarassing for us if we start off with libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp7WQLnn-cE8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf         # Open source machine learning library\n",
        "import numpy as np              # Python's scientific computing library\n",
        "import cv2                      # Open source computer vision platform\n",
        "\n",
        "from google.colab import drive  # For accessing data on Google Drive\n",
        "from google.colab import files  # File utilities for Google Colab\n",
        "import pickle as pkl            # For saving Python objects (classes)\n",
        "\n",
        "import os                       # File access utilities\n",
        "import matplotlib.pyplot as plt # Plotting and display utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MwC4aeh1YaL"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive to access our shared data!\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4noUrfHR1Or-"
      },
      "source": [
        "## Defining hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc3B7hOQ1Q6T"
      },
      "outputs": [],
      "source": [
        "PARAMS = {\n",
        "    'ROOT_PATH': '/content/drive/MyDrive/23AITHREAD_FaceRecDat',\n",
        "    'IMG_SIZE': (150, 150),\n",
        "    'EPOCHS': 10\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QAFYx0f0F1f"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez2fg3oyMeUa"
      },
      "outputs": [],
      "source": [
        "class FaceExtractor(object):\n",
        "    '''\n",
        "    Using OpenCV's cascade classifiers to detect faces\n",
        "    '''\n",
        "    def __init__(self, xml_path):\n",
        "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
        "\n",
        "    def detect(self, image):\n",
        "        scale_factor = 1.2\n",
        "        min_neighbors = 5\n",
        "        min_size = (30, 30)\n",
        "        face_coords = self.classifier.detectMultiScale(\n",
        "            image,\n",
        "            scaleFactor  = scale_factor,\n",
        "            minNeighbors = min_neighbors,\n",
        "            minSize      = min_size,\n",
        "            flags        = cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        return face_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-_MHUzMY2Db"
      },
      "outputs": [],
      "source": [
        "extractor = FaceExtractor(os.path.join(\n",
        "    PARAMS['ROOT_PATH'], 'haarcascade_frontalface_default.xml'\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Frnmn9qYzyu"
      },
      "outputs": [],
      "source": [
        "def crop(image, face_coords):\n",
        "    '''\n",
        "    Crops an image based on a list of coordinates.\n",
        "    Returns a list of cropped images (represented as pixel arrays)\n",
        "    '''\n",
        "\n",
        "    faces = []\n",
        "    for (x, y, w, h) in face_coords:\n",
        "        faces.append(image[y: y + h, x: x + w])\n",
        "    return faces\n",
        "\n",
        "def resize(images, size):\n",
        "    '''\n",
        "    Rezies a list of images to a given size\n",
        "    '''\n",
        "\n",
        "    resized_images = []\n",
        "\n",
        "    for img in images:\n",
        "        if img.shape < size:\n",
        "            interpolation = cv2.INTER_AREA\n",
        "        else:\n",
        "            interpolation = cv2.INTER_CUBIC\n",
        "\n",
        "        resz = cv2.resize(img, size, interpolation=interpolation)\n",
        "        resized_images.append(resz)\n",
        "\n",
        "    return resized_images\n",
        "\n",
        "def normalize(images):\n",
        "    '''\n",
        "    Maps all pixel values in an image to between -1 and 1\n",
        "    '''\n",
        "\n",
        "    return (images - 127.5) / 255\n",
        "\n",
        "def denormalize(images):\n",
        "    '''\n",
        "    Maps the pixel values of normalized image back to its orignal values\n",
        "    '''\n",
        "\n",
        "    return images * 255 + 127.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELmgQ0P6ag2A"
      },
      "outputs": [],
      "source": [
        "def preproc(image, size):\n",
        "    '''\n",
        "    Wrapper function that takes in a cv2 image, turns it into grayscale,\n",
        "    extracts faces from it, resizes these images, and then normalizes them\n",
        "    '''\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    face_coords = extractor.detect(gray)\n",
        "\n",
        "    faces = crop(gray, face_coords)\n",
        "\n",
        "    resized = np.array(resize(faces, size), np.float32)\n",
        "\n",
        "    normalized = normalize(resized)\n",
        "\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUUooVA2Ad8S"
      },
      "outputs": [],
      "source": [
        "def get_dataset(path, size):\n",
        "    '''\n",
        "    Creates a dataset from a given directory,\n",
        "    treating subdirectory as classes.\n",
        "\n",
        "    It applies `preproc` on all images, and returns\n",
        "    an array of images, an array of corresponding labels\n",
        "    (classes represented as integers), a mapping from\n",
        "    labels to class names, and the number of classes\n",
        "    '''\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    labels_dict = {}\n",
        "    people = sorted([person for person in os.listdir(path)])\n",
        "\n",
        "    for i, person in enumerate(people):\n",
        "        print('Processing images of {}'.format(person))\n",
        "\n",
        "        labels_dict[i] = person\n",
        "        for image_name in os.listdir(os.path.join(path,\n",
        "                                                       person)):\n",
        "            img = cv2.imread(\n",
        "                os.path.join(path, person, image_name), 1\n",
        "            )\n",
        "            faces = preproc(img, size)\n",
        "\n",
        "            for f in faces:\n",
        "                f = tf.expand_dims(f, -1)\n",
        "                if f.shape == (size[0], size[1], 1):\n",
        "                    images.append(f)\n",
        "                    labels.append(i)\n",
        "\n",
        "    return (np.asarray(images, np.float32),\n",
        "            np.asarray(labels, np.float32),\n",
        "            labels_dict,\n",
        "            len(people))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OrUc7diMs26"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels, labels_dict, num_ppl = get_dataset(\n",
        "    os.path.join(PARAMS['ROOT_PATH'], 'train'), PARAMS['IMG_SIZE']\n",
        ")\n",
        "\n",
        "print()\n",
        "print('Training data shape: {}'.format(train_data.shape))\n",
        "print('Training labels shape: {}'.format(train_labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtx1l18jAcX"
      },
      "source": [
        "## Model Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM3zEurejCPZ"
      },
      "source": [
        "### Architecture\n",
        "\n",
        "Here, we will design a standard convolution architecture. Refer to the diagram below if youre stuck!\n",
        "\n",
        "![ConvDiagram](https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru_-voIrLsgj"
      },
      "outputs": [],
      "source": [
        "# The complete convolutional model architecture\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters     = 128,\n",
        "        kernel_size = (5, 5),\n",
        "        activation  = 'relu',\n",
        "        input_shape = (PARAMS['IMG_SIZE'][0],\n",
        "                       PARAMS['IMG_SIZE'][1],\n",
        "                       1)\n",
        "    ),\n",
        "\n",
        "    tf.keras.layers.MaxPool2D(\n",
        "        pool_size = (2, 2)\n",
        "    ),\n",
        "\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters     = 64,\n",
        "        kernel_size = (3, 3),\n",
        "        activation  = 'relu'\n",
        "    ),\n",
        "\n",
        "    tf.keras.layers.MaxPool2D(\n",
        "        pool_size = (2, 2)\n",
        "    ),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(num_ppl, activation='softmax')\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amGqnGvMlkTI"
      },
      "outputs": [],
      "source": [
        "# Prints a summary of the model archiecture\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Ly2dEkk0c9"
      },
      "source": [
        "### Optimzers and Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF8JhuMYjEoV"
      },
      "outputs": [],
      "source": [
        "# Defines an optimzer and a loss function for the model\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss      = 'sparse_categorical_crossentropy',\n",
        "              metrics   = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyLD4M4Yk5tX"
      },
      "source": [
        "### Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j02ZtYLWjF-f"
      },
      "outputs": [],
      "source": [
        "# Trains the model on our data\n",
        "model.fit(train_data, train_labels, epochs = PARAMS['EPOCHS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84nEszkoC0rF"
      },
      "outputs": [],
      "source": [
        "# Saves the model weights and our predefined hyperparameters\n",
        "model.save('face.h5')\n",
        "\n",
        "file = open('face.aux', 'wb')\n",
        "pkl.dump((PARAMS, labels_dict), file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5BpPl52jJLo"
      },
      "outputs": [],
      "source": [
        "# Downloads files for standalone prediction\n",
        "files.download('face.aux')\n",
        "files.download('face.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cklRhaKdmOHV"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNIeT90NmYHA"
      },
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_data, test_labels, test_labels_dict, _ = get_dataset(\n",
        "    os.path.join(PARAMS['ROOT_PATH'], 'test'), PARAMS['IMG_SIZE']\n",
        ")\n",
        "\n",
        "print()\n",
        "print('Testing data shape: {}'.format(test_data.shape))\n",
        "print('Testing labels shape: {}'.format(test_labels.shape))\n",
        "\n",
        "assert test_labels_dict == labels_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhNwUIlmpg6"
      },
      "outputs": [],
      "source": [
        "# Evaluate model accuracy on unseen test data\n",
        "_ = model.evaluate(test_data, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O92t2IMXnyV3"
      },
      "outputs": [],
      "source": [
        "# Visualize model performance on the test dataset\n",
        "for sample_img, sample_label in zip(test_data, test_labels):\n",
        "\n",
        "    img_to_show = denormalize(sample_img)\n",
        "\n",
        "    plt.imshow(tf.squeeze(img_to_show), cmap='gray')\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "    predictions = model.predict(tf.expand_dims(sample_img, 0))\n",
        "    pred_id = np.argmax(predictions[0])\n",
        "    print(pred_id)\n",
        "\n",
        "    print('Predicted: ' + labels_dict[pred_id])\n",
        "    print('Label: ' + labels_dict[sample_label] + '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl7m6fMrp4Z2"
      },
      "source": [
        "## Standalone Webcam Prediction Code\n",
        "\n",
        "Once you've trained your model, how cool would it be to keep it forever and show off? Well, you're in treat, because this tool allows you to use your **webcam** to make predictions.\n",
        "\n",
        "Make sure you have your `face.aux` and `face.h5` available in your `/content` directory (the default directory when Colab starts up), and you're all set to go!\n",
        "\n",
        "*When you just want to make predictions on your webcam, you can simply run this cell on its own without anything before it (provided `face.aux` and `face.h5` are uploaded).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaQbxLXTp9qC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from IPython.display import display, Javascript, Image, clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pickle as pkl\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "PARAMS, labels_dict = pkl.load(open('face.aux', 'rb'))\n",
        "model = tf.keras.models.load_model('face.h5')\n",
        "\n",
        "class FaceExtractor(object):\n",
        "    def __init__(self, xml_path):\n",
        "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
        "\n",
        "    def detect(self, image):\n",
        "        scale_factor = 1.2\n",
        "        min_neighbors = 5\n",
        "        min_size = (30, 30)\n",
        "        face_coords = self.classifier.detectMultiScale(\n",
        "            image,\n",
        "            scaleFactor  = scale_factor,\n",
        "            minNeighbors = min_neighbors,\n",
        "            minSize      = min_size,\n",
        "            flags        = cv2.CASCADE_SCALE_IMAGE\n",
        "        )\n",
        "        return face_coords\n",
        "\n",
        "extractor = FaceExtractor(os.path.join(\n",
        "    PARAMS['ROOT_PATH'], 'haarcascade_frontalface_default.xml'\n",
        "))\n",
        "\n",
        "def crop(image, face_coords):\n",
        "    faces = []\n",
        "    for (x, y, w, h) in face_coords:\n",
        "        faces.append(image[y: y + h, x: x + w])\n",
        "    return faces\n",
        "\n",
        "def resize(images, size):\n",
        "    resized_images = []\n",
        "\n",
        "    for img in images:\n",
        "        if img.shape < size:\n",
        "            interpolation = cv2.INTER_AREA\n",
        "        else:\n",
        "            interpolation = cv2.INTER_CUBIC\n",
        "\n",
        "        resz = cv2.resize(img, size, interpolation=interpolation)\n",
        "        resized_images.append(resz)\n",
        "\n",
        "    return resized_images\n",
        "\n",
        "def normalize(images):\n",
        "    return (images - 127.5) / 255\n",
        "\n",
        "def denormalize(images):\n",
        "    return images * 255 + 127.5\n",
        "\n",
        "def preproc(image, size):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    face_coords = extractor.detect(gray)\n",
        "    faces = crop(gray, face_coords)\n",
        "    resized = np.array(resize(faces, size), np.float32)\n",
        "    normalized = normalize(resized)\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = 'Take photo';\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        div.remove();\n",
        "        return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "        ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "while 1:\n",
        "    clear_output()\n",
        "    filename = take_photo()\n",
        "\n",
        "    img = cv2.imread(filename, 1)\n",
        "    img = preproc(img, PARAMS['IMG_SIZE'])[0]\n",
        "    img_to_show = denormalize(img)\n",
        "\n",
        "    plt.imshow(tf.squeeze(img_to_show), cmap='gray')\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "    img = tf.reshape(img, (1, img.shape[0], img.shape[1], 1))\n",
        "    predictions = model.predict(img)\n",
        "    pred_id = np.argmax(predictions[0])\n",
        "    print('Prediction: ' + labels_dict[pred_id])\n",
        "\n",
        "    inp = input('\\nAgain? [Y]/n \\n')\n",
        "    if inp.lower() != 'y':\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Convolutional Face Recognition (Complete).ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
