{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap from Week 1\n",
    "\n",
    "- In this thread, we are interested in systems that exhibit <u>intelligent behaviour</u>\n",
    "\n",
    "- The modern paradigm of doing so is called <u>machine learning</u>\n",
    "\n",
    "- The standard machine learning pipeline is:\n",
    "    1. Define target behaviour in terms of inputs and outputs\n",
    "    2. Collect data of target behaviour\n",
    "    3. Initialise a **model**\n",
    "    4. **Train modelon** data to predict the **correct** outputs from inputs\n",
    "    5. **Evaluate** model performance\n",
    "\n",
    "Our question this week is:\n",
    "> How do we build a **model**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a model?\n",
    "\n",
    "* A model can be thought of as a function $\\mathbf{y} = f(\\mathbf{x})$. It takes in an input $\\mathbf{x}$ and spits out an output $\\mathbf{y}$.\n",
    "\n",
    "* We want the model to <u>approximate</u> our data, predicting the correct output $\\mathbf{y}$.\n",
    "\n",
    "> The challenge is to a come up with a **general architecture** that can **approximate any function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "* Neural networks are one example of such *general architectures*. \n",
    "\n",
    "* They are composed of <u>perceptrons</u>.\n",
    "\n",
    "![Deep Neural Network](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/cdp/cf/ul/g/3a/b8/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork.png)\n",
    "\n",
    "*Image courtesy of IBM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Suppose you have a collection of inputs $\\mathbf{x} = (x_1, x_2, x_3, ..., x_n)$.\n",
    "\n",
    "A perceptron multiplies each input by a <u>weight</u> and sums the products:\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
    "$$\n",
    "\n",
    "> **Note:** *The* $w_0$ *term is called the* <u>bias term</u>.\n",
    "\n",
    "![A perceptron](https://miro.medium.com/v2/resize:fit:1400/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n",
    "\n",
    "*Image courtesy of \"What the Hell is Perceptron? The Fundamentals of Neural Networks\", SAGAR SHARMA, Towards Data Science, Sep 9, 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many Perceptrons\n",
    "\n",
    "We can scale up to many perceptrons and many layers, where each perceptron in a layer takes in all outputs from the previous layer as input. This is called a <u>multilayer perceptron (MLP)</u>, and is one type of <u>deep neural networks</u>:\n",
    "\n",
    "![Multilayer perceptron](https://d3i71xaburhd42.cloudfront.net/2589b72d4e928896dc668a24839de4f2adcc6726/11-Figure3-1.png)\n",
    "\n",
    "*Image courtesy of Shao, Changpeng. “A Quantum Model for Multilayer Perceptron.” arXiv: Quantum Physics (2018): n. pag.*\n",
    "\n",
    "### In vector form...\n",
    "\n",
    "$$\n",
    "y = \\mathbf{W}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "But successive perceptron layers is identical to just one!\n",
    "\n",
    "$$\n",
    "z = \\mathbf{W_n}...\\mathbf{W_2}\\mathbf{W_1}\\mathbf{x} = (\\mathbf{W_n}...\\mathbf{W_2}\\mathbf{W_1})\\mathbf{x} = \\mathbf{W_{supreme}}\\mathbf{x}\n",
    "$$\n",
    "\n",
    "The solution? <u>Activation functions!</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "These inject <u>non-linearity</u> into the perceptron output, so that the MLP can learn more complex relationships:\n",
    "\n",
    "$$\n",
    "y = \\sigma(\\mathbf{W}\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem\n",
    "\n",
    "Neural networks have some very interesting <u>universal approximation</u> results:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
